{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_rnn_withCustomEncoder.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "_2VQo4bajwUU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN for Tensor Flow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "My initial work looked at the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis. This was using in the tensor flow examples so looked to be a good place to start. However, I discovered the [Toxic Comment challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) and that provided a much more relavent dataset. \n",
        "\n",
        "The code is based on a number of examples from TensorFlow combined with my own code and some ideas from [Pukar Acharya](https://github.com/iampukar/toxic-comments-classification/blob/master/toxic_comment_analysis.ipynb)\n",
        "\n",
        "See https://github.com/Workshopshed/TinyMLTextClassification\n",
        "\n",
        "This notebook trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the dataset provided for the competition.\n",
        "\n",
        "**Caution:** their data set intentionally contains comments that are both toxic and obsecene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z682XYsrjkY9",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import pandas as pd\n",
        "import numpy as numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Note, you'll need to upload the data sets from the competition to colab or wherever you are running this notebook from. Note that pandas (pd) is expecting the files to be in UTF-8 form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHRwRoP2nVHX",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('train_small.csv')\n",
        "test_data = pd.read_csv('test_small.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOJ_IXPgbMP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phiOIgaLbSuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTjPT02bmCn",
        "colab_type": "text"
      },
      "source": [
        "Validate that the data does not have any blanks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVfPCobnbo48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSK-SsbTb5AW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "result_set = train_data[classes].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oRU_OiEb-hY",
        "colab_type": "text"
      },
      "source": [
        "And the input data from the comment_text column of the CSV, make it all lowercase to simplify the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZquWwm_-cCuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences = train_data[\"comment_text\"].fillna(\"fillna\").str.lower()\n",
        "test_sentences = test_data[\"comment_text\"].fillna(\"fillna\").str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MCorLciXSDJE"
      },
      "source": [
        "Create our custom encoder based on `tfds.features.text.TextEncoder`. Note this is more complex than it needs to be because of the orgional processing of data via tfds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQEY0xNToMM0",
        "colab": {}
      },
      "source": [
        "import binascii\n",
        "import sys\n",
        "import tensorflow_text as text\n",
        "from math import floor\n",
        "\n",
        "class HashedTextEncoder(tfds.features.text.TextEncoder):\n",
        "  \"\"\"Encodes text using PySuperFastHash\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Constructs HashedTextEncoder.\n",
        "    Args:\n",
        "      None\n",
        "    \"\"\"\n",
        "  def encode(self, s):\n",
        "    # Handle additional tokens\n",
        "    s = tf.compat.as_text(s)\n",
        "    s = s.lower()\n",
        "    ids = []\n",
        "    words = s.split(\" \") \n",
        "    for substr in words[0:8]:\n",
        "      if not substr:\n",
        "        continue\n",
        "      newid = self.superFastHash(substr)\n",
        "      ids.append(newid)\n",
        "    #If length is too long then select the middle words\n",
        "    #if len(ids) > 12:\n",
        "    #  ids = ids[floor((len(ids)-12) / 2):floor((len(ids)-12) / 2) + 12]\n",
        "    return self.pad_incr(ids)\n",
        "\n",
        "  def pad_incr(self,ids):\n",
        "    \"\"\"Add 1 to ids to account for pad.\"\"\"\n",
        "    return [i + 1 for i in ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def load_from_file():\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def save_to_file():\n",
        "    raise NotImplementedError  \n",
        "  \n",
        "  def vocab_size():\n",
        "    raise NotImplementedError  \n",
        "\n",
        "  def get16bits(self, data):\n",
        "    \"\"\"Returns the first 16bits of a string\"\"\"\n",
        "    return int(binascii.hexlify(data[1::-1]), 16)\n",
        "\n",
        "  def superFastHash(self, data):\n",
        "    # Start by stripping out UTF data\n",
        "    data=data.encode(\"ascii\",\"ignore\")\n",
        "\n",
        "    hash = length = len(data)\n",
        "    if length == 0:\n",
        "        return 0\n",
        "\n",
        "    rem = length & 3\n",
        "    length >>= 2\n",
        "\n",
        "    while length > 0:\n",
        "        hash += self.get16bits(data) & 0xFFFFFFFF\n",
        "        tmp = (self.get16bits(data[2:])<< 11) ^ hash\n",
        "        hash = ((hash << 16) & 0xFFFFFFFF) ^ tmp\n",
        "        data = data[4:]\n",
        "        hash += hash >> 11\n",
        "        hash = hash & 0xFFFFFFFF\n",
        "        length -= 1\n",
        "\n",
        "    if rem == 3:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 16) & 0xFFFFFFFF\n",
        "        hash ^= (data[2] << 18) & 0xFFFFFFFF\n",
        "        hash += hash >> 11\n",
        "    elif rem == 2:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 11) & 0xFFFFFFFF\n",
        "        hash += hash >> 17\n",
        "    elif rem == 1:\n",
        "        hash += data[0]\n",
        "        hash ^= (hash << 10) & 0xFFFFFFFF\n",
        "        hash += hash >> 1\n",
        "\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 3) & 0xFFFFFFFF\n",
        "    hash += hash >> 5\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 4) & 0xFFFFFFFF\n",
        "    hash += hash >> 17\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 25) & 0xFFFFFFFF\n",
        "    hash += hash >> 6\n",
        "\n",
        "    return hash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAfGg8YRe6fu"
      },
      "source": [
        "This text encoder converts words to hashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq6xDmf2SAs-",
        "colab": {}
      },
      "source": [
        "#Needed to test filtering out unicode strings\n",
        "sample_string = 'Hello TensorFlow, this is a @fun test. Fichier non trouvé, now check that the too long didnt read function is also working'\n",
        "\n",
        "encoder = HashedTextEncoder()\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "#Note this is a hash function so we can't reverse the operation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GlYWqhTVlUyQ"
      },
      "source": [
        "## Prepare the data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw_iwRZkTk7L",
        "colab_type": "text"
      },
      "source": [
        "Now run the encoder on the dataset, pad to 8 numbers and format for processing by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poSSXmzedur2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentences(sentences):\n",
        "\n",
        "  outputarray = []\n",
        "\n",
        "  for words in sentences:\n",
        "    row = encoder.encode(words)\n",
        "    # Padd the rows to 8 so we can stack them for Numpy / Keras\n",
        "    x = len(row)\n",
        "    while x < 8:\n",
        "      row.append(0)\n",
        "      x = x + 1\n",
        "    outputarray.append(row)\n",
        "  return outputarray\n",
        "\n",
        "encoded_sentences = encode_sentences(train_sentences)\n",
        "\n",
        "for i in encoded_sentences[:5]:\n",
        "  print('Encoded string is {}'.format(i))\n",
        "\n",
        "train_batches = numpy.stack( encoded_sentences,axis=0 )\n",
        "\n",
        "encoded_test = encode_sentences(test_sentences)\n",
        "test_batches = numpy.stack(encoded_test,axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4V02jLwj6Yd",
        "colab_type": "text"
      },
      "source": [
        "Check the shape of the data, it should match the shape of the model inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUMhnfm8j_H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_batches.shape)\n",
        "print(test_batches.shape)\n",
        "print(result_set.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Build a `tf.keras.Sequential` model, the first embedding layer needs to be as big as our biggest hash + 1 as 0 is used for padding.\n",
        "\n",
        "When using the Embedding layer, the input_length parameter is needed so that we don't get the following error when converting the model to lite.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "`None is only supported in the 1st dimension. Tensor 'embedding_input' has invalid shape '[None, None]'.`\n",
        "\n",
        "Otherwise use the input_shape parameter\n",
        "\n",
        "A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input—and then to the next.\n",
        "\n",
        "The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwfoBkmRYcP3",
        "colab": {}
      },
      "source": [
        "# Orgional Model.\n",
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(32767,64, input_length=32),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "#    tf.keras.layers.Dense(64, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1)\n",
        "#])\n",
        "\n",
        "# This one seems to cause problems with optimisation steps.\n",
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(1025,16, input_length=16),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "#    tf.keras.layers.Dense(16, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1)\n",
        "#])\n",
        "\n",
        "# Experiments\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(8, activation='relu',input_shape=(8,)),                       \n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(6)\n",
        "  ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:\n",
        "\n",
        "Optimisers - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj2xei41YZjC",
        "colab": {}
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI5shHP1xfxA",
        "colab_type": "text"
      },
      "source": [
        "Note that the validation split allows some of the training data to be kept bac for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mj6ClREIh3-2",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_batches,result_set, epochs=30, batch_size=10, validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaNbXi43YgUT",
        "colab": {}
      },
      "source": [
        "#TODO: Need to fix this\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0W_jiReyE0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "model.save(\"textclassification_model\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DwSE_386uhxD"
      },
      "source": [
        "The above model does not mask the padding applied to the sequences. This can lead to skew if trained on padded sequences and test on un-padded sequences. Ideally you would [use masking](../../guide/keras/masking_and_padding) to avoid this, but as you can see below it only have a small effect on the output.\n",
        "\n",
        "If the prediction is >= 0.5, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ARZI6Ks_au",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w0dseJMiEUh",
        "colab": {}
      },
      "source": [
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y-E4cgkIvmVu",
        "colab": {}
      },
      "source": [
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 8)\n",
        "\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "  return (predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O41gw3KfWHus",
        "colab": {}
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=False)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFh4xLARucTy",
        "colab": {}
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "sample_pred_text = ('The movie was fantastic. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie. Loved every minute of it. A cast of famous people')\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "print(predictions)\n",
        "\n",
        "print(sample_predict('This was rubbish, wont be going again. Hated it. Totally pants', pad=True))\n",
        "\n",
        "print(sample_predict('Amazing film, loved seeing this', pad=True))\n",
        "\n",
        "print(sample_predict('Only a total noob would say something like that', pad=True))\n",
        "\n",
        "print(sample_predict('Have you considered an alternative', pad=True))\n",
        "\n",
        "print(sample_predict('Total crap, wont be going again', pad=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yROjuhJYsxJw",
        "colab_type": "text"
      },
      "source": [
        "# Export the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQoXPUlhDbT3",
        "colab_type": "text"
      },
      "source": [
        "Convert the model to TFLite then format as a big C array.\n",
        "\n",
        "Based on https://github.com/eloquentarduino/tinymlgen/blob/master/tinymlgen/tinymlgen.py\n",
        "\n",
        "Ref https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbyfLSen6S8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hexdump"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb13ix0DkJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Experimenting with optimisations\n",
        "\n",
        "import re\n",
        "import hexdump\n",
        "import tensorflow as tf\n",
        "\n",
        "def port(model,optimize=True, variable_name='model_data',pretty_print=False):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    if optimize:\n",
        "        if isinstance(optimize, bool):\n",
        "            optimizers = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "        else:\n",
        "            optimizers = optimize\n",
        "\n",
        "        converter.optimizations = optimizers\n",
        "    tflite_model = converter.convert()\n",
        "    bytes = hexdump.dump(tflite_model).split(' ')\n",
        "    c_array = ', '.join(['0x%02x' % int(byte, 16) for byte in bytes])\n",
        "    c = 'const unsigned char %s[] DATA_ALIGN_ATTRIBUTE = {%s};' % (variable_name, c_array)\n",
        "    if pretty_print:\n",
        "        c = c.replace('{', '{\\n\\t').replace('}', '\\n}')\n",
        "        c = re.sub(r'(0x..?, ){12}', lambda x: '%s\\n\\t' % x.group(0), c)\n",
        "    c += '\\nconst int %s_len = %d;' % (variable_name, len(bytes))\n",
        "    preamble = '''\n",
        "#ifdef __has_attribute\n",
        "#define HAVE_ATTRIBUTE(x) __has_attribute(x)\n",
        "#else\n",
        "#define HAVE_ATTRIBUTE(x) 0\n",
        "#endif\n",
        "#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))\n",
        "#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\n",
        "#else\n",
        "#define DATA_ALIGN_ATTRIBUTE\n",
        "#endif\n",
        "'''\n",
        "    return preamble + c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-A2h4mD0jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_code = port(model,optimize=True,pretty_print=True)\n",
        "\n",
        "print(len(c_code))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIRo66RoTc-1",
        "colab_type": "text"
      },
      "source": [
        "File size needs to be < 400K to fit onto the device. Check the model_data_len value at the bottom of the file.\n",
        "const int model_data_len = 109840 and a tiny bit of code comes to 90% of the availabe space.\n",
        "But perhaps it also needs to be smaller than the available ram to be able to run? For example the sine model is just 2640 bytes;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UumRfLYuFBxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_file = open(r\"text_model.h\",\"w+\")\n",
        "\n",
        "n = c_file.write(c_code)\n",
        "c_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NgZhT8EaSc",
        "colab_type": "text"
      },
      "source": [
        "# Testing the TFLite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EKHsYKEm-w",
        "colab_type": "text"
      },
      "source": [
        "It is possible to reload the model back into the notebook and test it here.\n",
        "\n",
        "Arena Size?\n",
        "\n",
        "https://github.com/edgeimpulse/tflite-find-arena-size\n",
        "\n",
        "\n",
        "Debugging TFLite\n"
      ]
    }
  ]
}