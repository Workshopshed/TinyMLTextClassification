{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_rnn_withCustomEncoder.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "_2VQo4bajwUU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "0nbI5DtDGw-i",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN for Tensor Flow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z682XYsrjkY9",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* dataset—all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHRwRoP2nVHX",
        "colab": {}
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, \n",
        "                          as_supervised=True)\n",
        "train_examples, test_examples = dataset['train'], dataset['test']\n",
        "\n",
        "for ex in train_examples.take(4):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MCorLciXSDJE"
      },
      "source": [
        "Create our custom encoder based on `tfds.features.text.TextEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQEY0xNToMM0",
        "colab": {}
      },
      "source": [
        "import binascii\n",
        "import sys\n",
        "import tensorflow_text as text\n",
        "\n",
        "class HashedTextEncoder(tfds.features.text.TextEncoder):\n",
        "  \"\"\"Encodes text using PySuperFastHash\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Constructs HashedTextEncoder.\n",
        "    Args:\n",
        "      None\n",
        "    \"\"\"\n",
        "  def encode(self, s):\n",
        "    # Handle additional tokens\n",
        "    s = tf.compat.as_text(s)\n",
        "    s = s.lower()\n",
        "    ids = []\n",
        "    words = s.split(\" \") \n",
        "    for substr in words:\n",
        "      if not substr:\n",
        "        continue\n",
        "      newid = self.superFastHash(substr)\n",
        "      ids.append(newid)\n",
        "    return self.pad_incr(ids)\n",
        "\n",
        "  def pad_incr(self,ids):\n",
        "    \"\"\"Add 1 to ids to account for pad.\"\"\"\n",
        "    return [i + 1 for i in ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def load_from_file():\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def save_to_file():\n",
        "    raise NotImplementedError  \n",
        "  \n",
        "  def vocab_size():\n",
        "    raise NotImplementedError  \n",
        "\n",
        "  def get16bits(self, data):\n",
        "    \"\"\"Returns the first 16bits of a string\"\"\"\n",
        "    return int(binascii.hexlify(data[1::-1]), 16)\n",
        "\n",
        "  def superFastHash(self, data):\n",
        "    # Start by stripping out UTF data\n",
        "    data=data.encode(\"ascii\",\"ignore\")\n",
        "\n",
        "    hash = length = len(data)\n",
        "    if length == 0:\n",
        "        return 0\n",
        "\n",
        "    rem = length & 3\n",
        "    length >>= 2\n",
        "\n",
        "    while length > 0:\n",
        "        hash += self.get16bits(data) & 0xFFFFFFFF\n",
        "        tmp = (self.get16bits(data[2:])<< 11) ^ hash\n",
        "        hash = ((hash << 16) & 0xFFFFFFFF) ^ tmp\n",
        "        data = data[4:]\n",
        "        hash += hash >> 11\n",
        "        hash = hash & 0xFFFFFFFF\n",
        "        length -= 1\n",
        "\n",
        "    if rem == 3:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 16) & 0xFFFFFFFF\n",
        "        hash ^= (data[2] << 18) & 0xFFFFFFFF\n",
        "        hash += hash >> 11\n",
        "    elif rem == 2:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 11) & 0xFFFFFFFF\n",
        "        hash += hash >> 17\n",
        "    elif rem == 1:\n",
        "        hash += data[0]\n",
        "        hash ^= (hash << 10) & 0xFFFFFFFF\n",
        "        hash += hash >> 1\n",
        "\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 3) & 0xFFFFFFFF\n",
        "    hash += hash >> 5\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 4) & 0xFFFFFFFF\n",
        "    hash += hash >> 17\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 25) & 0xFFFFFFFF\n",
        "    hash += hash >> 6\n",
        "\n",
        "    #Shorter version throw away top bits\n",
        "    hash = hash & 0x1FFF\n",
        "\n",
        "    return hash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAfGg8YRe6fu"
      },
      "source": [
        "This text encoder converts words to hashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq6xDmf2SAs-",
        "colab": {}
      },
      "source": [
        "#Needed to test filtering out unicode strings\n",
        "sample_string = 'Hello TensorFlow, this is a @fun test. Fichier non trouvé'\n",
        "\n",
        "encoder = HashedTextEncoder()\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "#Note this is a hash function so we can't reverse the operation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GlYWqhTVlUyQ"
      },
      "source": [
        "## Prepare the data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw_iwRZkTk7L",
        "colab_type": "text"
      },
      "source": [
        "Now run the encoder on the dataset by wrapping it in `tf.py_function` and passing that to the dataset's map method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80O8ZJNTmlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzYDEHh5TuIc",
        "colab_type": "text"
      },
      "source": [
        "You want to use `Dataset.map` to apply this function to each element of the dataset. `Dataset.map` runs in graph mode.\n",
        "\n",
        "*   Graph tensors do not have a value.\n",
        "*   In graph mode you can only use TensorFlow Ops and functions.\n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV_T6QseVNOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label\n",
        "\n",
        "\n",
        "train_encoded = train_examples.map(encode_map_fn)\n",
        "test_encoded = test_examples.map(encode_map_fn)\n",
        "\n",
        "train_batches = train_encoded.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "test_batches = test_encoded.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9L8OQ5HVpwQ",
        "colab_type": "text"
      },
      "source": [
        "Lets take a look at one of these to see how it now looks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vvru8bMikpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_example, train_label in train_batches.take(1):\n",
        "  print('Encoded text:', train_example[:10].numpy())\n",
        "  print('Label:', train_label.numpy())\n",
        "\n",
        "for example_batch, label_batch in train_batches.take(2):\n",
        "  print(\"Batch shape:\", example_batch.shape)\n",
        "  print(\"label shape:\", label_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Build a `tf.keras.Sequential` model, the first embedding layer needs to be as big as our biggest hash + 1 as 0 is used for padding. The input_length parameter is needed so that we don't get the following error when converting the model to lite\n",
        "\n",
        "`None is only supported in the 1st dimension. Tensor 'embedding_input' has invalid shape '[None, None]'.`\n",
        "\n",
        "A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input—and then to the next.\n",
        "\n",
        "The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwfoBkmRYcP3",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(32767,64, input_length=32),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "#    tf.keras.layers.Dense(64, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1)\n",
        "#])\n",
        "\n",
        "# Simpler model from https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(8193, 16,input_length=16),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj2xei41YZjC",
        "colab": {}
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI5shHP1xfxA",
        "colab_type": "text"
      },
      "source": [
        "Training on a reduced data set to speed verification of the technique. Can use the whole set once we know the process will work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hw86wWS4YgR2",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_batches.take(30), epochs=1,\n",
        "                    validation_data=test_batches.take(30), \n",
        "                    validation_steps=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaNbXi43YgUT",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0W_jiReyE0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "model.save(\"saved_model/textclassification_model\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DwSE_386uhxD"
      },
      "source": [
        "The above model does not mask the padding applied to the sequences. This can lead to skew if trained on padded sequences and test on un-padded sequences. Ideally you would [use masking](../../guide/keras/masking_and_padding) to avoid this, but as you can see below it only have a small effect on the output.\n",
        "\n",
        "If the prediction is >= 0.5, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ARZI6Ks_au",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w0dseJMiEUh",
        "colab": {}
      },
      "source": [
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y-E4cgkIvmVu",
        "colab": {}
      },
      "source": [
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "  return (predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O41gw3KfWHus",
        "colab": {}
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=False)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFh4xLARucTy",
        "colab": {}
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "sample_pred_text = ('The movie was fantastic. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie. Loved every minute of it. A cast of famous people')\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "print(predictions)\n",
        "\n",
        "sample_pred_text2 = ('This was rubbish, wont be going again. Hated it. Totally pants')\n",
        "predictions = sample_predict(sample_pred_text2, pad=True)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yROjuhJYsxJw",
        "colab_type": "text"
      },
      "source": [
        "# Export the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQoXPUlhDbT3",
        "colab_type": "text"
      },
      "source": [
        "Install TinyML Gen to convert the model to C format. Is internally using the TFLiteConverter to create the TFLite image and then formatting it as a big C array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb13ix0DkJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tinymlgen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-A2h4mD0jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tinymlgen import port\n",
        "\n",
        "c_code = port(model,optimize=True,pretty_print=True)\n",
        "\n",
        "print(len(c_code))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGMl6Hf1SkMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The c_code string was too big to dump to the screen so instead we save it to our G Drive \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UumRfLYuFBxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_file = open(r\"/content/gdrive/My Drive/text_model.h\",\"w+\")\n",
        "\n",
        "n = c_file.write(c_code)\n",
        "c_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}