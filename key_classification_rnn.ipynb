{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "key_classification_rnn.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "_2VQo4bajwUU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "0nbI5DtDGw-i",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN for Tensor Flow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis.\n",
        "\n",
        "It was based on a number of examples from TensorFlow combined with my own code. See "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z682XYsrjkY9",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* dataset—all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHRwRoP2nVHX",
        "colab": {}
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, \n",
        "                          as_supervised=True)\n",
        "train_examples, test_examples = dataset['train'], dataset['test']\n",
        "\n",
        "for ex in train_examples.take(4):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MCorLciXSDJE"
      },
      "source": [
        "Create our custom encoder based on `tfds.features.text.TextEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQEY0xNToMM0",
        "colab": {}
      },
      "source": [
        "import binascii\n",
        "import sys\n",
        "import tensorflow_text as text\n",
        "from math import floor\n",
        "\n",
        "class HashedTextEncoder(tfds.features.text.TextEncoder):\n",
        "  \"\"\"Encodes text using PySuperFastHash\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"Constructs HashedTextEncoder.\n",
        "    Args:\n",
        "      None\n",
        "    \"\"\"\n",
        "  def encode(self, s):\n",
        "    # Handle additional tokens\n",
        "    s = tf.compat.as_text(s)\n",
        "    s = s.lower()\n",
        "    ids = []\n",
        "    words = s.split(\" \") \n",
        "    for substr in words[0:16]:\n",
        "      if not substr:\n",
        "        continue\n",
        "      newid = self.superFastHash(substr)\n",
        "      ids.append(newid)\n",
        "    #If length is too long then select the middle words\n",
        "    #if len(ids) > 12:\n",
        "    #  ids = ids[floor((len(ids)-12) / 2):floor((len(ids)-12) / 2) + 12]\n",
        "    return self.pad_incr(ids)\n",
        "\n",
        "  def pad_incr(self,ids):\n",
        "    \"\"\"Add 1 to ids to account for pad.\"\"\"\n",
        "    return [i + 1 for i in ids]\n",
        "\n",
        "  def decode(self, ids):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def load_from_file():\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def save_to_file():\n",
        "    raise NotImplementedError  \n",
        "  \n",
        "  def vocab_size():\n",
        "    raise NotImplementedError  \n",
        "\n",
        "  def get16bits(self, data):\n",
        "    \"\"\"Returns the first 16bits of a string\"\"\"\n",
        "    return int(binascii.hexlify(data[1::-1]), 16)\n",
        "\n",
        "  def superFastHash(self, data):\n",
        "    # Start by stripping out UTF data\n",
        "    data=data.encode(\"ascii\",\"ignore\")\n",
        "\n",
        "    hash = length = len(data)\n",
        "    if length == 0:\n",
        "        return 0\n",
        "\n",
        "    rem = length & 3\n",
        "    length >>= 2\n",
        "\n",
        "    while length > 0:\n",
        "        hash += self.get16bits(data) & 0xFFFFFFFF\n",
        "        tmp = (self.get16bits(data[2:])<< 11) ^ hash\n",
        "        hash = ((hash << 16) & 0xFFFFFFFF) ^ tmp\n",
        "        data = data[4:]\n",
        "        hash += hash >> 11\n",
        "        hash = hash & 0xFFFFFFFF\n",
        "        length -= 1\n",
        "\n",
        "    if rem == 3:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 16) & 0xFFFFFFFF\n",
        "        hash ^= (data[2] << 18) & 0xFFFFFFFF\n",
        "        hash += hash >> 11\n",
        "    elif rem == 2:\n",
        "        hash += self.get16bits (data)\n",
        "        hash ^= (hash << 11) & 0xFFFFFFFF\n",
        "        hash += hash >> 17\n",
        "    elif rem == 1:\n",
        "        hash += data[0]\n",
        "        hash ^= (hash << 10) & 0xFFFFFFFF\n",
        "        hash += hash >> 1\n",
        "\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 3) & 0xFFFFFFFF\n",
        "    hash += hash >> 5\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 4) & 0xFFFFFFFF\n",
        "    hash += hash >> 17\n",
        "    hash = hash & 0xFFFFFFFF\n",
        "    hash ^= (hash << 25) & 0xFFFFFFFF\n",
        "    hash += hash >> 6\n",
        "\n",
        "    #Shorter version throw away top bits\n",
        "    hash = hash & 0x3FF\n",
        "\n",
        "    return hash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAfGg8YRe6fu"
      },
      "source": [
        "This text encoder converts words to hashes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq6xDmf2SAs-",
        "colab": {}
      },
      "source": [
        "#Needed to test filtering out unicode strings\n",
        "sample_string = 'Hello TensorFlow, this is a @fun test. Fichier non trouvé, now check that the too long didnt read function is also working'\n",
        "\n",
        "encoder = HashedTextEncoder()\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "#Note this is a hash function so we can't reverse the operation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GlYWqhTVlUyQ"
      },
      "source": [
        "## Prepare the data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw_iwRZkTk7L",
        "colab_type": "text"
      },
      "source": [
        "Now run the encoder on the dataset by wrapping it in `tf.py_function` and passing that to the dataset's map method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80O8ZJNTmlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzYDEHh5TuIc",
        "colab_type": "text"
      },
      "source": [
        "You want to use `Dataset.map` to apply this function to each element of the dataset. `Dataset.map` runs in graph mode.\n",
        "\n",
        "*   Graph tensors do not have a value.\n",
        "*   In graph mode you can only use TensorFlow Ops and functions.\n",
        "\n",
        "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function.\n",
        "\n",
        "Note that this means there is delayed execution of these functions and you don't see them run until you access them or process them through a model.\n",
        "\n",
        "Question: Can we down size the int here or do we wait till the quantisation step? https://stackoverflow.com/questions/22725043/convert-dtype-from-int64-to-int32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV_T6QseVNOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_map_fn(text, label):\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int64))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiKPTKQRtH7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "train_encoded = train_examples.map(encode_map_fn)\n",
        "test_encoded = test_examples.map(encode_map_fn)\n",
        "\n",
        "train_batches = train_encoded.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "test_batches = test_encoded.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9L8OQ5HVpwQ",
        "colab_type": "text"
      },
      "source": [
        "Lets take a look at one of these to see how it now looks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vvru8bMikpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_example, train_label in train_batches.take(1):\n",
        "  print('Encoded text:', train_example[:10].numpy())\n",
        "  print('Label:', train_label.numpy())\n",
        "\n",
        "for example_batch, label_batch in train_batches.take(1):\n",
        "  print(\"Batch shape:\", example_batch.shape)\n",
        "  print(\"label shape:\", label_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Build a `tf.keras.Sequential` model, the first embedding layer needs to be as big as our biggest hash + 1 as 0 is used for padding.\n",
        "\n",
        "When using the Embedding layer, the input_length parameter is needed so that we don't get the following error when converting the model to lite.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "`None is only supported in the 1st dimension. Tensor 'embedding_input' has invalid shape '[None, None]'.`\n",
        "\n",
        "Otherwise use the input_shape parameter\n",
        "\n",
        "A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input—and then to the next.\n",
        "\n",
        "The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the output. This helps the RNN to learn long range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwfoBkmRYcP3",
        "colab": {}
      },
      "source": [
        "# Orgional Model.\n",
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(32767,64, input_length=32),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "#    tf.keras.layers.Dense(64, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1)\n",
        "#])\n",
        "\n",
        "# This one seems to cause problems with optimisation steps.\n",
        "# model = tf.keras.Sequential([\n",
        "#    tf.keras.layers.Embedding(1025,16, input_length=16),\n",
        "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "#    tf.keras.layers.Dense(16, activation='relu'),\n",
        "#    tf.keras.layers.Dense(1)\n",
        "#])\n",
        "\n",
        "# Experiment that does not seem to work, can't get our tensors in the right shape for bidirectional\n",
        "# Could it be done by modifying the map function?\n",
        "# model = tf.keras.Sequential([\n",
        "#  tf.keras.layers.Reshape((-1,3),input_shape=(16,1)),\n",
        "#  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "#  tf.keras.layers.Dense(64, activation='relu'),\n",
        "#  tf.keras.layers.Dense(1)\n",
        "#  ])\n",
        "\n",
        "# Simpler model from https://www.tensorflow.org/tutorials/keras/text_classification\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(1025, 8,input_length=16),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(8, activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that we choose to Keras sequential model here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:\n",
        "\n",
        "Optimisers - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj2xei41YZjC",
        "colab": {}
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI5shHP1xfxA",
        "colab_type": "text"
      },
      "source": [
        "Training on a reduced data by using .take(20) on the batches and reducing the validation steps to speed verification of the technique. Can use the whole set once we know the process will work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hw86wWS4YgR2",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_batches, epochs=30,\n",
        "                    validation_data=test_batches.take(20), \n",
        "                    validation_steps=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaNbXi43YgUT",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0W_jiReyE0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "model.save(\"textclassification_model\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DwSE_386uhxD"
      },
      "source": [
        "The above model does not mask the padding applied to the sequences. This can lead to skew if trained on padded sequences and test on un-padded sequences. Ideally you would [use masking](../../guide/keras/masking_and_padding) to avoid this, but as you can see below it only have a small effect on the output.\n",
        "\n",
        "If the prediction is >= 0.5, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ARZI6Ks_au",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w0dseJMiEUh",
        "colab": {}
      },
      "source": [
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y-E4cgkIvmVu",
        "colab": {}
      },
      "source": [
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 16)\n",
        "\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "  return (predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O41gw3KfWHus",
        "colab": {}
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=False)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kFh4xLARucTy",
        "colab": {}
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "sample_pred_text = ('The movie was fantastic. The animation and the graphics '\n",
        "                    'were out of this world. I would recommend this movie. Loved every minute of it. A cast of famous people')\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "print(predictions)\n",
        "\n",
        "print(sample_predict('This was rubbish, wont be going again. Hated it. Totally pants', pad=True))\n",
        "\n",
        "print(sample_predict('Amazing film, loved seeing this', pad=True))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yROjuhJYsxJw",
        "colab_type": "text"
      },
      "source": [
        "# Export the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQoXPUlhDbT3",
        "colab_type": "text"
      },
      "source": [
        "Convert the model to TFLite then format as a big C array.\n",
        "\n",
        "Based on https://github.com/eloquentarduino/tinymlgen/blob/master/tinymlgen/tinymlgen.py\n",
        "\n",
        "Ref https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbyfLSen6S8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hexdump"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcb13ix0DkJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Experimenting with optimisations\n",
        "\n",
        "import re\n",
        "import hexdump\n",
        "import tensorflow as tf\n",
        "\n",
        "def port(model,optimize=True, variable_name='model_data',pretty_print=False):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    if optimize:\n",
        "        if isinstance(optimize, bool):\n",
        "            optimizers = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "        else:\n",
        "            optimizers = optimize\n",
        "\n",
        "        converter.optimizations = optimizers\n",
        "    tflite_model = converter.convert()\n",
        "    bytes = hexdump.dump(tflite_model).split(' ')\n",
        "    c_array = ', '.join(['0x%02x' % int(byte, 16) for byte in bytes])\n",
        "    c = 'const unsigned char %s[] DATA_ALIGN_ATTRIBUTE = {%s};' % (variable_name, c_array)\n",
        "    if pretty_print:\n",
        "        c = c.replace('{', '{\\n\\t').replace('}', '\\n}')\n",
        "        c = re.sub(r'(0x..?, ){12}', lambda x: '%s\\n\\t' % x.group(0), c)\n",
        "    c += '\\nconst int %s_len = %d;' % (variable_name, len(bytes))\n",
        "    preamble = '''\n",
        "#ifdef __has_attribute\n",
        "#define HAVE_ATTRIBUTE(x) __has_attribute(x)\n",
        "#else\n",
        "#define HAVE_ATTRIBUTE(x) 0\n",
        "#endif\n",
        "#if HAVE_ATTRIBUTE(aligned) || (defined(__GNUC__) && !defined(__clang__))\n",
        "#define DATA_ALIGN_ATTRIBUTE __attribute__((aligned(4)))\n",
        "#else\n",
        "#define DATA_ALIGN_ATTRIBUTE\n",
        "#endif\n",
        "'''\n",
        "    return preamble + c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-A2h4mD0jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_code = port(model,optimize=True,pretty_print=True)\n",
        "\n",
        "print(len(c_code))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIRo66RoTc-1",
        "colab_type": "text"
      },
      "source": [
        "File size needs to be < 400K to fit onto the device. Check the model_data_len value at the bottom of the file.\n",
        "const int model_data_len = 109840 and a tiny bit of code comes to 90% of the availabe space.\n",
        "But perhaps it also needs to be smaller than the available ram to be able to run? For example the sine model is just 2640 bytes;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UumRfLYuFBxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_file = open(r\"text_model.h\",\"w+\")\n",
        "\n",
        "n = c_file.write(c_code)\n",
        "c_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NgZhT8EaSc",
        "colab_type": "text"
      },
      "source": [
        "# Testing the TFLite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EKHsYKEm-w",
        "colab_type": "text"
      },
      "source": [
        "It is possible to reload the model back into the notebook and test it here.\n",
        "\n",
        "Arena Size?\n",
        "\n",
        "https://github.com/edgeimpulse/tflite-find-arena-size\n",
        "\n",
        "\n",
        "Debugging TFLite\n"
      ]
    }
  ]
}
